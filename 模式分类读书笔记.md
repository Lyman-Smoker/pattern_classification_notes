## Chapter 4 No parametric Techniques

#### 4.5.4 k-邻近规则

k-邻近规则是在最邻近规则的基础上推广得来的。**这个规则将一个测试数据点x分类为与其最接近的k个近邻中出现最多的那个类别**。

<img src=".\images\image-20211004154208658.png" alt="image-20211004154208658" style="zoom:33%;" />

根据这种规则，只有当k个近邻中的大多数被标记为$w_m$，才将样本x判决为类别$w_m$。做出这样选择的概率为：$\sum^k_{i=(k+1)/2}(^k_i)p(w_m|x)^i[1-p(w_m|x)]^{k-i}$。另外，通过分析误差率可以证明，在**训练样本充足的情况下**，当k增加时，误差率将渐渐逼近贝叶斯误差率。

<img src=".\images\image-20211004154958387.png" alt="image-20211004154958387" style="zoom:50%;" />

事实上，k-近邻规则可以被看做是另一种从样本中估计后验概率$P(w_i|x)$的方法。为了得到可靠的估计，我们希望k越大越好。但是另一方面，我们又希望x的k个近邻x'距离x越近越好没因为这样可以保证$p(w_i|x')$尽可能地逼近$p(w_i|x)$。因此，在实际应用中，由于训练样本数量的限制，k趋近于无穷的时候$p(w_i|x)$将趋于先验概率，这可能导致不够准确，因此**需要对k做某中折中，控制在某个范围从而使k-近邻算法的效果最好**。

#### 4.5.5 k-近邻规则的计算复杂度

首先假设我们在d维空间上对复杂度进行讨论。

若现有n个训练样本，需要对一个测试样本进行最邻近计算。显然，每计算一个距离的时间复杂度为$O(d)$。考虑一个最简单的情形，我们需要遍历所有的训练样本，并且从中找出距离最近的那个，那么搜索的总时间复杂度为$O(dn^2)$。（其实是不是可以牺牲一点空间复杂度将其降到$O(dn)$？）

要降低该复杂度，大体上有三种方法：计算部分距离(partial distance)、预建立结构(presturcturing )和对训练样本加以剪辑(editing the stored prototypes)。

对于partial distance，我们选取d维中的r个子维度进行距离计算，如果效果不错，那么就不进一步进行计算。部分距离的计算方法为：$D_r(a,b)=(\sum^r_{k=1}(a_k-b_k)^2)^{1/2}$，其中$r<d$。

在presturcturing中，思路是通过建立一个搜索树，用树形结构来降低计算的复杂度。

editing the stored prototypes方法事实上就是在训练的过程中有选择地消去无用的训练样本（剪枝）。P.S：这个地方有点没看懂，需要后续回顾。

在实际的应用中，三种方法可以混合使用。

### 4.6 距离度量和最近邻分类

#### 4.6.1 度量的性质

首先需要明确的是我们可以选取什么用的度量来评价距离。根据过往的经验，一个度量必须满足以下四个性质：

- 非负性
- 自反性：$D(a,b)=0$ if and only if $a=b$
- 对称性
- 三角不等式：$D(a,b)+D(b,c)≥D(a,c)$

一个非常常用的度量方法是采用范数。我们将k范数定义为：$L_k=(\sum^d_{i=1}|a_i-b_i|^k)^{1/k}$。

<img src=".\images\image-20211012173642155.png" alt="image-20211012173642155" style="zoom: 67%;" />

此外，Tanimoto距离度量在分类学上也有广泛的应用。对于两个集合$S_1、S_2$来说，$D_{Tanimoto}(S_1,S_2)=\frac{n_1+n_2-2n_{12}}{n_1+n_2-n_{12}}$ 。（不重合部分占总部分的比例）

#### 4.6.1 切距（Tangent distance）

如果我们不加约束地随便选用一个距离度量来进行实践，那么就会造成很不好的效果。书中举的例子如下图所示：

![image-20211012185900805](.\images\image-20211012185900805.png)

对于一个标识10*10手写体数字5的实例x'，如果使用欧氏距离作为距离度量，那么当这个5向右平移3个单位后得到的x'(s=3)与x'之间的距离将比一个手写数字8的实例x_8还要大。这显然是不合理的。类似的例子还有很多。

因此，书中在这里提出了切距分类器（tangent distance classifier）。其基本的思想是使用一个全新的距离度量和一个对随机变化的线性逼近。

假设我们知道目前处理的问题存在r中变换，那么在设计分类器的时候，我们首先对每个样本点x‘进行每一种变换操作$F_i(x';α_i)$。然后为每一种变换构建一个切向量$TV_i=F(x';α_i)-x'$。如此，对于一个样本点x'，我们就可以构造一个包含所有切向量的r*d矩阵T。

假设这些切向量是线性无关的，那么这些切向量一定可以在样本空间中构造出一个超平面（切平面）。这个切平面表征的是x'做各种变换后得到的点可能落在的区域。那么对于一个测试样本，我们只需要计算这个测试样本到切平面之间的距离就可以评估其与原型数据x’之间的相近程度了。

<img src=".\images\image-20211012193859651.png" alt="image-20211012193859651" style="zoom: 67%;" />

## Chapter 5 Lineaer Discriminant Functions

### 5.2 线性判别函数和决策平面

#### 5.2.1 二分类

先看线性判别函数的定义：

<img src=".\images\image-20211012195754320.png" alt="image-20211012195754320" style="zoom:80%;" />

对于一个二分类问题（含类w_1和w_2），线性判别函数的判别规则可以被描述为：
$$
x =\begin{cases}w1 & g(x) > 0\\w2 & g(x) < 0\\random & g(x)=0\end{cases}
$$
书中还有一些基本的描述，由于比较简单这里就不过多阐述，可以直接看书。其中我认为比较有意思的一个描述是这个：

<img src=".\images\image-20211012201915786.png" alt="image-20211012201915786" style="zoom:80%;" />

#### 5.2.2 多分类

